# gcp-ml-api-deployment
Deployment project of fine-tune a 7B parameter LLM AI model, exposed as a production-ready API endpoint on GCP using Docker and FastAPI.

# GCP ML API Deployment â€“ Falcon-7B LoRA Adapter  
Deploying a fine-tuned 7B LLM as a production-ready REST API using Google Cloud, Docker, and FastAPI.

This project demonstrates the full lifecycle of taking a large-scale language model (Falcon-7B-Instruct), fine-tuning it using LoRA, and deploying it as a callable API endpoint on a CPU-only Google Cloud VM.  
It includes training, optimization, containerization, networking, and API exposure.

---

## ğŸ“Œ Project Repository  
GitHub: https://github.com/Rahul446K/gcp-ml-api-deployment.git

---

# ğŸš€ Overview

The primary goal of this project is to fine-tune a 7B parameter model and run it in a **resource-constrained CPU environment**.  
Since 7B models normally require a GPU, several optimizations were introduced:

- Training LoRA adapters on **Kaggle GPU** due to GCP GPU unavailability  
- Running inference on **CPU** using optimized memory loading  
- Utilizing **FastAPI** to expose model inference as REST endpoints  
- Packaging the entire service in **Docker** for reproducibility  
- Hosting on **GCP Compute Engine** with proper firewall & networking setup  

Even on CPU, the API is operational and suitable for demo, testing, and prototyping (response time 1â€“3 minutes).

---

# ğŸ§  Model Details

| Component | Value |
|----------|-------|
| Base Model | `tiiuae/falcon-7b-instruct` |
| Fine-tuning | LoRA adapters (trained via Kaggle + Tinker workflow) |
| Serving Hardware | GCP Compute Engine (CPU only) |
| Format | PyTorch |
| Frameworks | transformers, peft, accelerate, torch |

---

# ğŸ—ï¸ Architecture

Client â†’ FastAPI â†’ Docker Container â†’ Falcon-7B Model + LoRA â†’ CPU Inference â†’ Response

---

# ğŸ“š Training Workflow (Kaggle GPU)

1. Prepared dataset & notebook for LoRA fine-tuning  
2. Enabled **Kaggle GPU (T4)**  
3. Fine-tuned Falcon-7B using low-rank adapters  
4. Exported `lora-devotee/` directory  
5. Added LoRA weights to Docker image for deployment  

---

# â˜ï¸ GCP Deployment

### VM Details
- OS: Ubuntu 22.04  
- Machine Type: e2-standard series  
- Port Exposed: **8000**  
- Authentication: SSH keys  
- Firewall: Custom ingress rule allowing TCP:8000  

---

# ğŸ› ï¸ Setup & Installation

### 1. Clone the repository
```bash
git clone https://github.com/Rahul446K/gcp-ml-api-deployment.git
cd gcp-ml-api-deployment
```

### 2. Optional: Create a virtual environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Run locally (without Docker)
```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

---

# ğŸ³ Setup & Installation (Using Docker)

### 1. Build the Docker Image
```bash
docker build -t devotee-api .
```

### 2. Run the API Container
```bash
docker run -d --name ai_chat_service -p 8000:8000 devotee-api
```

### 3. Verify the Deployment
```bash
curl http://34.60.125.198:8000/
```

### 4. Test Chat Endpoint
```bash
curl -X POST "http://34.60.125.198:8000/chat"      -H "Content-Type: application/json"      -d '{"prompt":"Hare Krishna, please guide me."}'
```

---

# ğŸ”§ Environment Variables

Create `.env`:

```
HOST=0.0.0.0
PORT=8000
MODEL_NAME=tiiuae/falcon-7b-instruct
LORA_DIR=./lora-devotee
DEVICE=cpu
MAX_TOKENS=120
TEMPERATURE=0.7
TOP_P=0.9
```

---

# ğŸŒ Public API Base URL

```
http://34.60.125.198:8000
```

---

# ğŸ”Œ API Endpoints

## 1ï¸âƒ£ Health Check â€“ GET `/`
```
http://34.60.125.198:8000/
```

Response:
```json
{"message": "Devotee Chat API running!"}
```

---

## 2ï¸âƒ£ Chat Inference â€“ POST `/chat`
```
http://34.60.125.198:8000/chat
```

Request:
```json
{"prompt": "Hare Krishna, please guide me."}
```

âš ï¸ Response time on CPU: **1â€“3 minutes**

---

## 3ï¸âƒ£ Swagger API Documentation â€“ `/docs`
```
http://34.60.125.198:8000/docs
```

---

# ğŸ›‘ Troubleshooting

| Issue | Reason | Fix |
|-------|--------|------|
| OOM error (Out Of Memory) | Model too large for RAM | Used accelerate offloading |
| Git auth failed | Password login deprecated | Use SSH key or PAT |
| Port unreachable | Firewall closed | Allow TCP:8000 |
| Docker build slow | Large model files | Expected (10â€“20 mins) |
| `/chat` not working in browser | Only accepts POST | Use curl or Postman |

---

# ğŸ“ˆ Future Enhancements

- Deploy on GPU VM for fast inference  
- Add request streaming  
- Add authentication & rate limits  
- Quantize model (4-bit GGUF)  
- Serve via vLLM or TGI for high throughput  

---

# ğŸ“„ License  
MIT License  
---

# âœ¨ Author  
**Rahul Kumar**
